---
output:
  html_document: default
  pdf_document: default
---
# Assignment 1
## Biomedical Data Science
## B159640

### Problem 1

Loading library to work with data tables
```{r}
library(data.table)
```

Loading the electronic health record data from the study cohort
```{r}
cohort <- read.csv("assignment1_data/cohort.csv", stringsAsFactors = TRUE)
lab <- read.csv("assignment1_data/lab1.csv")
linker <- read.csv("assignment1_data/linker.csv", stringsAsFactors = TRUE)
```

#### Problem 1 (a)

Converting data frames to data tables and exclude the `yob` field in `cohort` to ensure that only one yob field remains in the final data table
```{r}
cohort.dt <- data.table(cohort)
lab.dt <- data.table(lab)[,!"yob"] 
linker.dt <- data.table(linker)
```

Merging `cohort` and `lab` dataset using the `linker` dataset to create a single data table based dataset `cohort.dt`
```{r}
cohort.dt <- merge(cohort.dt, linker.dt, by.x = "id", by.y = "id")
cohort.dt <- merge(cohort.dt, lab.dt, by.x = "LABID", by.y = "LABID")
```

Ensuring that the `albumin` field is converted to a factor:
We ensured that when reading the `.csv` files into R and specified `stringsAsFactors = TRUE`, but we will check it again:
```{r}
is.factor(cohort.dt$albumin)
```

Ensuring that the ordering of the factor is 1=“normo”,2=“micro”,3=“macro”:
```{r}
cohort.dt$albumin <- relevel(cohort.dt$albumin, "macro")
cohort.dt$albumin <- relevel(cohort.dt$albumin, "micro")
cohort.dt$albumin <- relevel(cohort.dt$albumin, "normo")

table(cohort.dt$albumin)
```

Ensuring the `diabetes` field is field is converted to a factor, too:
```{r}
cohort.dt[, diabetes := as.factor(diabetes)]
```

Performing assertion checks:

* to ensure that all identifiers in cohort.csv have been accounted for
* to ensure that any validation fields are consistent between sets
```{r}
stopifnot(all(cohort.dt$id %in% cohort$id))

setorder(cohort.dt, id)
setorder(linker.dt, id)
stopifnot(all(cohort.dt[,id, LABID] == linker.dt[,id, LABID]))
```

Dropping the identifier that originated from lab dataset LABID
```{r}
cohort.dt$LABID <- NULL
```

#### Problem 1 (b)

Calculating the age/year offset:
```{r}
age.offset <- mean(cohort.dt$yob + cohort.dt$age, na.rm=TRUE)
```

Updating any missing age fields based on age/year offset:
```{r}
cohort.dt[, age := ifelse(is.na(age), age.offset-yob, age)]
```

#### Problem 1 (c) and (d)

Creating a separate data table to provide a summary of the data in the `cohort.dt` table.
It contains a row for each column in the `cohort.dt` table and the following columns:

* variable name
* Median (Interquartile Range) for continuous non-categorical data and N (%) for cate-
gorical
* Total N
* Missing N (%)

Creating a function that takes a vector of numeric or integer values and returns a character string of the median and interquartile range. The functions accepts a boolean parameter `impute.to.mean` to impute any missing values to the mean value of the vector. Its default value is set to `FALSE`:
```{r}
return.median.plus.iqr <- function(vector, impute.to.mean=FALSE){
  
  if(impute.to.mean==TRUE){
    # find missing values
    na.idx <- is.na(vector)
    # replace NAs with the median
    vector[na.idx] <- mean(vector, na.rm=TRUE)
  }
  
  # calculate the mean
  v.mean <- round( median(vector, na.rm = TRUE),2)
  # calculate the interquartile range
  v.iqr <- round( quantile(vector, c(0.25, 0.75), na.rm = TRUE),2)
  # format output so that it is a string with form “12 (11,13)”
  output <- paste0(v.mean, " (", v.iqr[1],",",v.iqr[2],")")
  
  return(output)
}
```

Performing the function on all `cohort.dt` columns with continuous values and using the result to calculate the summary table for all continuous variables:
```{r}
summary.con.variables <- data.table("Variable" = colnames(cohort.dt[, .SD, .SDcols = sapply(cohort.dt, is.numeric)]),
                                    "Median (IQR) or N (%)" = sapply(cohort.dt[, .SD, .SDcols = sapply(cohort.dt, is.numeric)], return.median.plus.iqr),
                                    "Total N" = sapply(cohort.dt[, .SD, .SDcols = sapply(cohort.dt, is.numeric)], length),
                                    "Missing N (%)" = sapply(cohort.dt[, .SD, .SDcols = sapply(cohort.dt, is.numeric)], function(z)sum(is.na(z))/length(z)*100))
```

Splitting categorical fields in the summary table into a row per category. Creating a function that takes a vector of categorical values as an input and returns the total number of non-missing rows of the respective factor and the overall percentage of the respective factor:
```{r}
return.summary.cat <- function(vector, use.str){
  
  # calculate total number of non-missing rows of the factor category
  cat.N <- length( vector[which(vector == use.str )])
  # calculate overall percentage of non-missing rows of the factor category based on the input vector
  cat.perc <- round( cat.N / length(vector) * 100, 2 )
   # format output so that it is a string with form “12 (5%)”
  output <- paste0( cat.N, " (", cat.perc, ")")
  return(output)
}
```

Using this function to compute the total number of non-missing rows of the `albumin` and `diabetes` categories and the overall percentage of the `albumin` and `diabetes` categories:
```{r}
per.cat.N <- c( return.summary.cat(cohort.dt$diabetes, 0),
                return.summary.cat(cohort.dt$diabetes, 1),
                return.summary.cat(cohort.dt$albumin, "normo"),
                return.summary.cat(cohort.dt$albumin, "micro"),
                return.summary.cat(cohort.dt$albumin, "macro"))
```

Computing the remaining values for the summary table of the categorial variables
```{r}
cat.missing.N <- format(round(sapply(cohort.dt[, c("diabetes","albumin")], function(z)sum(is.na(z))/length(z)*100),2), nsmall = 2)
cat.total.N <- sapply(cohort.dt[, c("diabetes","albumin")], length)
```

Creating the summary table for all non-continuous variables:
```{r}
summary.cat.variables <- data.table("Variable" = c("diabetes (0)","diabetes(1)","albumin (normo)","albumin (micro)","albumin (macro)"),
                                    "Median (IQR) or N (%)" = per.cat.N,
                                    "Total N" = c(rep(cat.total.N[1],2),rep(cat.total.N[2],3)),
                                    "Missing N (%)" = c(rep(cat.missing.N[1],2),rep(cat.missing.N[2],3)))
```

Combining the results in one summary data table listing both the continuous and categorical variables:
```{r}
summary.cohort.dt <- rbind(summary.con.variables, summary.cat.variables)
summary.cohort.dt
```

### Problem 2

Loading the `data.table` library and importing the required data sets
```{r}
library(data.table)

gfr.1 <- read.csv("assignment1_data/ltegfr1.csv", stringsAsFactors = TRUE)
gfr.2 <- read.csv("assignment1_data/ltegfr2.csv", stringsAsFactors = TRUE)
```

#### Problem 2 (a)

Converting the files to data tables and merge in an appropriate way into a single data table
```{r}
eGFR.dt <- merge( data.table(gfr.1),
                 data.table(gfr.2),
                 by.x = c("id","fu.years"), 
                 by.y = c("ID","fu.years"),
                 all = TRUE )
```

Ordering the observations according to subject identifier and follow-up time
```{r}
setorderv(eGFR.dt, c("id","fu.years"), c(1,1))
```

Adding an assertion that the ordering is correct

```{r}
stopifnot( all(eGFR.dt[, c("id", "fu.years")] == eGFR.dt[order(id, fu.years), fu.years, by = id] ))
```

#### Problem 2 (b)

Computing the average eGFR and length of follow-up for each patient:
```{r}
summary.egfr.table <- NULL

for (pat.id in 1:max(eGFR.dt$id)){
  
  # calculate average eGFR for each patient
  avg.egfr <- mean(eGFR.dt[id == pat.id]$egfr, na.rm = TRUE)
  
  # calculate average follow-up for each patient
  avg.fu.years <- mean(eGFR.dt[id == pat.id]$fu.years, na.rm = TRUE)
  
  # create a summary table
  summary.egfr.table <- rbind(summary.egfr.table, c(pat.id, avg.egfr, avg.fu.years))
}

colnames(summary.egfr.table) <- c("pat.id", "avg.gfr", "avg.fu.years") # assign colnames to table
summary.egfr.dt <- data.table(summary.egfr.table) # create data.table
head(summary.egfr.dt)
```

Tabulating the number of patients with average eGFR in the following ranges:
(0, 15], (15, 30], (30, 60], (60, 90], (90, ∞)
```{r}
egfr.bins <- c(0, 15, 30, 60, 90, Inf)
table(cut(summary.egfr.dt$avg.gfr, egfr.bins))
```

Counting and reporting the number of patients with missing average eGFR:
```{r}
# number of patients with missing average eGFR
sum(is.na(summary.egfr.dt$avg.gfr))
```

Ensuring that the table ordering is returned to id, and follow-up time:
Check `eGFR.dt` and `summary.egfr.dt` table:
```{r}
# eGFR.dt
setorderv(eGFR.dt, c("id","fu.years"), c(1,1))
stopifnot( all(eGFR.dt[, c("id", "fu.years")] == eGFR.dt[order(id, fu.years), fu.years, by = id] ))

# summary.egfr.dt
setorderv(summary.egfr.dt, c("pat.id","avg.fu.years"), c(1,1))
stopifnot( all(summary.egfr.dt[, c("pat.id", "avg.fu.years")] == summary.egfr.dt[order(pat.id, avg.fu.years), avg.fu.years, by = pat.id]))
```

Fitting a linear regression model for the eGFR measurements as a function of time 
for each patient with at least 3 eGFR readings and store it in the `summary.egfr.dt` data table
```{r}
# create empty vector
lr.models <- vector(mode="list", length=max(eGFR.dt$id))

# loop over all patients in eGFR data table
for (pat.id in 1:max(eGFR.dt$id)){
  
  # check whether patient has at least 3 eGFR readings
  if (sum(!is.na(eGFR.dt[id == pat.id]$egfr)) >= 3){
    # fit LR model
    regr.egfr <- lm(egfr ~  fu.years, data = eGFR.dt[id == pat.id], na.action = na.omit)
  } else {
    regr.egfr <- NA
  }
  
  lr.models[[pat.id]] <- regr.egfr
  
}

# add LR models to the summary data table
summary.egfr.dt <- cbind(summary.egfr.dt, lr.models)
```

Counting how many patients have a slope < -3, [-3, 0), [0, 3], > 3
```{r}
# Creating slope bins
slope.1 <- 0 # < -3
slope.2 <- 0 # [-3, 0)
slope.3 <- 0 # [0, 3]
slope.4 <- 0 # > 3

# Looping over entries of summary data table
for (id in 1:max(summary.egfr.dt$pat.id)){
  
  # check whether LR model exists
  if (!is.na(summary.egfr.dt[pat.id == id ]$lr.models)){
    
    # assign slope of the model to variable
    slope <- coef(summary.egfr.dt[pat.id == id ]$lr.models[[1]])[2]
    
    # increment the respective bin counter by 1
    if (slope < -3){
      slope.1 <- slope.1 + 1
    } else if (slope >= -3 && slope < 0){
      slope.2 <- slope.2 + 1
    } else if (slope >= 0 && slope <= 3){
      slope.3 <- slope.3 + 1
    } else if (slope > 3){
      slope.4 <- slope.4 + 1
    }
  }
}

# create data table
slopes.dt <- data.table("slope" = c("< -3","[-3, 0)","[0, 3]","> 3"), 
                        "N Patients" = c(slope.1, slope.2, slope.3, slope.4))
slopes.dt
```

#### Problem 2 (c)

For patients with average eGFR in the (0,15] range, collecting their identifier, sex, age at baseline, average eGFR, time of last eGFR reading and number of eGFR measurements taken in a data table:
```{r}
eGFR.le.15 <- NULL

for (i in 1:max(summary.egfr.dt$pat.id)){
  avg.gfr <- summary.egfr.dt[pat.id == i]$avg.gfr

  if (!is.na(avg.gfr) && avg.gfr > 0 && avg.gfr <= 15){
    sex <- eGFR.dt[id == i]$sex[1]
    age <- eGFR.dt[id == i]$baseline.age[1]
    last.egfr <- last(eGFR.dt[id == i]$fu.years)
    egfr.measurements <- sum(!is.na(eGFR.dt[id == i]$egfr))
    
    eGFR.le.15 <- rbind(eGFR.le.15, c(i, sex, age, avg.gfr, last.egfr, egfr.measurements))

  }
  
}

eGFR.le.15.dt <- data.table(eGFR.le.15)
colnames(eGFR.le.15.dt) <- c('Patient ID', 'Sex', 'Age (baseline)', 'average eGFR', 'last eGFR measurments', 'eGFR measurements')

head(eGFR.le.15.dt)
```

#### Problem 2 (d)

For patients 3, 37, 162 and 223 (one at a time):

* ploting the patient’s eGFR measurements as a function of time
* fitting a linear regression model and add the regression line to the plot
* reporting the 95% confidence interval for the regression coefficients of the fitted model
* using a different colour, plot a second regression line computed after removing the
extreme eGFR values (one each of the highest and the lowest value)

```{r}
ids <- c(3, 37, 162, 223)

for(pat in ids){
  
  time <- eGFR.dt[id == pat]$fu.years
  egfr <- eGFR.dt[id == pat]$egfr
  
  time <- time[which(!is.na(egfr))]
  egfr <- egfr[!is.na(egfr)]
  
  # plotting the patient’s eGFR measurements over time
  plot(time, egfr,
       type = "l",
       col = "blue",
       main = paste0('PatientID ', pat ,': eGFR measurements over time'),
       xlab = 'time (years)',
       ylab = 'eGFR (mL/min/1.73 m2)')
  
  # fitting a linear regression model
  fit1 <- lm(egfr ~ time)
  
  # adding regression line to the plot
  abline(coef(fit1)[1], coef(fit1)[2], col="red")
  
  # fitting a 2nd linear regression model without extrem eGFR values
  time <- time[which(egfr > min(egfr) & egfr < max(egfr))]
  egfr <- egfr[egfr > min(egfr) & egfr < max(egfr)]
  
  fit2 <- lm(egfr ~ time)
  
  # adding 2nd regression line to the plot
  abline(coef(fit2)[1], coef(fit2)[2], col="orange")
  
  # legend to do differ fit1 from fit2
  redorange <- c("red","orange")
  legend("bottomleft", c("before removal", "after removal"), fill = redorange)
  
  # reporting  95% CI for the regression coefficients of fit 1
  print("95% CI for regression coefficients before removal")
  print(confint(fit1))
  
  # reporting  95% CI for the regression coefficients of fit 2
  print("95% CI for regression coefficients after removal")
  print(confint(fit2))
  
}
```

### Problem 3

#### Problem 3 (a)

Writing an R6 class `Calcegfr` that accepts 4 vectors: serum creatinine (`scr`), age (`age`), sex (`sex`) and ethnicity (`ethnic`). 
The initialisation method of the class validates that vector lengths match and stores them for use by the methods. The class then implements two separate public methods of the two eGFR formulas, MDRD4 and CKD-EPI, and stores the result as a vector of the same length as the input.

```{r}
library(R6)

Calcegfr <- R6Class("Calcegfr", public = list(
  scr = vector(),
  age = vector(),
  sex = factor(),
  ethnic = factor(),
  mdrd4.egfr = vector(),
  ckdepi.egfr = vector(),
  
  # initalisation method
  initialize = function(scr, age, sex, ethnic){
    
    # input data checks
    
    # check whether vector lengths match
    stopifnot(length(scr) == length(age) &&
              length(scr) == length(sex) &&
              length(scr) == length(ethnic))
    
    # check whether scr and age are numeric
    stopifnot(is.numeric(scr) && is.numeric(age))
    
    # check whether sex and ethnic are factors 
    stopifnot(is.factor(sex) && is.factor(ethnic))
    
    # initialise member variables
    
    self$scr <- scr
    self$age <- age
    self$sex <- sex
    self$ethnic <- ethnic
    
    self$mdrd4.egfr <- numeric(length = length(self$scr))
    self$ckdepi.egfr <- numeric(length = length(self$scr))
    
  }, 
  
  # estimating GFR with MDRD4 equation
  
  egfr.mdrd4 = function(){
    scr <- self$scr
    age <- self$age
    sex <- self$sex
    ethnic <- self$ethnic
    
    # converting levels of factor variables
    
    sex <- ifelse( sex == "Female", 0.742, 1)
    ethnic <- ifelse( ethnic == "Black", 1.212, 1)
    
    # main body
    
    self$mdrd4.egfr <- 175*(scr/88.42)^(-1.154)*age^(-0.203)*sex*ethnic
    
    # return
    return(self$mdrd4.egfr)
  },
  
  # estimating GFR with CKD-EPI equation
  
  egfr.ckdepi = function(){
    scr <- self$scr
    age <- self$age
    sex <- self$sex
    ethnic <- self$ethnic
    
    # calculating alpha and kappa
    
    alpha <- ifelse( sex == "Female", -0.329, -0.411)
    kappa <- ifelse( sex == "Female", 0.7, 0.9)
    
    # converting levels of factor variables

    sex <- ifelse( sex == "Female", 1.018, 1)
    ethnic <- ifelse( ethnic == "Black", 1.159, 1)
  
    # main body
    
    self$ckdepi.egfr <- 141*pmin((scr/88.42)/kappa, 1)^alpha*pmax((scr/88.42)/kappa, 1)^(-1.209)*0.993^(age)*sex*ethnic
    
    # return results
    return(self$ckdepi.egfr)
  }
    
))
```

#### Problem 3 (b)

Loading the `scr2.csv` dataset, creating a `Calcegfr` object and computing the eGFR according to the MDRD4 and CKD-EPI equations:
```{r}
# load data set
scr2 <- read.csv("assignment1_data/scr2.csv", stringsAsFactors = TRUE)

# create an Calcegfr object
scr2.calcegfr <- Calcegfr$new(scr2$scr, scr2$age, scr2$sex, scr2$ethnic)

# compute the eGFR according to the two equations
mdrd4.egfr <- scr2.calcegfr$egfr.mdrd4()
ckdepi.egfr <- scr2.calcegfr$egfr.ckdepi()
```

Computing mean and SD of the MDRD4 and CKD EPI results (rounded to the second decimal place):
```{r}
# mean of MDRD4 results
round(mean(mdrd4.egfr, na.rm = TRUE),2)
# SD of MDRD4 results
round(sd(mdrd4.egfr, na.rm = TRUE),2)

# mean of CKD-EPI results
round(mean(ckdepi.egfr, na.rm = TRUE),2)
# SD of CKD-EPI results
round(sd(ckdepi.egfr, na.rm = TRUE),2)
```
Computing Pearson's correlation coefficient between both eGFR results
```{r}
round(cor(mdrd4.egfr, ckdepi.egfr, method = "pearson", use = "complete.obs"),2)
```

Reporting the mean and SD for all patients with eGFRs between 0-60, 60-90 and > 90 according to MDRD4:
```{r}
# create eGFR bins
egfr.bins <- c(0, 60, 90, Inf)

# subset MDRD results according to bins
mdrd4.egfr.low <- mdrd4.egfr[which(cut(mdrd4.egfr, egfr.bins) == "(0,60]")]
mdrd4.egfr.med <- mdrd4.egfr[which(cut(mdrd4.egfr, egfr.bins) == "(60,90]")]
mdrd4.egfr.high <- mdrd4.egfr[which(cut(mdrd4.egfr, egfr.bins) == "(90,Inf]")]

# calculate mean and SD for all 3 bins
low.mean <- round(mean(mdrd4.egfr.low, na.rm = TRUE),2)
low.SD <- round(sd(mdrd4.egfr.low, na.rm = TRUE),2)

med.mean <- round(mean(mdrd4.egfr.med, na.rm = TRUE),2)
med.SD <- round(sd(mdrd4.egfr.med, na.rm = TRUE),2)

high.mean <- round(mean(mdrd4.egfr.high, na.rm = TRUE),2)
high.SD <- round(sd(mdrd4.egfr.high, na.rm = TRUE),2)

# create sumamry table of MDRD4 eGFR
mdrd4.summary.dt <- data.table("eGFR" = c("(0,60]","(60,90]", "(90,Inf]"),
                               "N" = c(length(mdrd4.egfr.low), length(mdrd4.egfr.med), length(mdrd4.egfr.high)),
                               "Mean" = c(low.mean, med.mean, high.mean),
                               "SD" = c(low.SD, med.SD, high.SD))

mdrd4.summary.dt
```
#### Problem 3 (c)

Producing a scatter plot of the two eGFR vectors with added vertical lines corresponding to median, first and third quartiles of the MDRD4 eGFR and horizontal lines corresponding to median, first and third quartiles of the CKD-EPI eGFR: 
```{r}
# Scatterplot of the MDRD4 and CKD-EPI eGFR
plot(mdrd4.egfr, ckdepi.egfr,
     cex = .5, 
     col='orange',
     main = paste("Q-Q Plot eGFR equations", 
                  "\nPearson's r: ", round(cor(mdrd4.egfr, ckdepi.egfr, method = "pearson", use = "complete.obs"),2)),
     xlab = "MDRD4",
     ylab = "CKD-EPI")

# add vertical lines corresponding to median, first and third quartiles of MDRD4
abline(v = median(mdrd4.egfr, na.rm = TRUE))
abline(v = quantile(mdrd4.egfr, na.rm = TRUE)[2])
abline(v = quantile(mdrd4.egfr, na.rm = TRUE)[4])

# add horizontal lines corresponding to median, first and third quartiles of CKD-EPI
abline(h = median(ckdepi.egfr, na.rm = TRUE))
abline(h = quantile(ckdepi.egfr, na.rm = TRUE)[2])
abline(h = quantile(ckdepi.egfr, na.rm = TRUE)[4])
```

**Is the relationship between the two eGFR equations linear?**


Yes. The Q-Q plot (see above) clearly shows a strong overall linear relationship between the MDRD4 and CKD-EPI eGFRs. The pearson correlation coefficient of 0.97 confirms this observation. However, for eGFR values greater than 90mL/min/1.73m2 dispersion of the points increases, indicating a weaker linear relationship above this threshold. To assess the linear relationship between the two eGFR equations below and above the threshold independently, I split the two eGFR vectors, produced two separate scatterplots and recalculated Pearson's r for both splits:
```{r}
par(mfrow=c(1,2))

plot(mdrd4.egfr[which(mdrd4.egfr < 90)], ckdepi.egfr[which(mdrd4.egfr < 90)],
     cex = .5, 
     col='orange',
     main = paste("Q-Q Plot eGFR equations", 
                  "\nPearson's r: ", round(cor(mdrd4.egfr[which(mdrd4.egfr < 90)], ckdepi.egfr[which(mdrd4.egfr < 90)], method = "pearson", use = "complete.obs"),2)),
     xlab = "MDRD4",
     ylab = "CKD-EPI")

plot(mdrd4.egfr[which(mdrd4.egfr > 90)], ckdepi.egfr[which(mdrd4.egfr > 90)],
     cex = .5, 
     col='orange',
     main = paste("Q-Q Plot eGFR equations", 
                  "\nPearson's r: ", round(cor(mdrd4.egfr[which(mdrd4.egfr > 90)], ckdepi.egfr[which(mdrd4.egfr > 90)], method = "pearson", use = "complete.obs"),2)),
     xlab = "MDRD4",
     ylab = "CKD-EPI")
```

This analysis revealed that a considerably weaker linear relationship (Pearson's r = 0.84) above the threshold of 90mL/min/1.73m2 (compared to r = 1.0 below the threshold). 
A closer look at the Q-Q Plots also revealed that the dispersion of points is the least for eGFRs close to zero and increases with the eGFR. Hence, I concluded that the relationsship between the MDRD4 and CKD-EPI eGFR equations is linear, but the strength of the linear relationsship declines with higher eGFR values.

### Problem 4

#### Problem 4 (a)

Loading the built-in `infert` dataset and converting it to data table
```{r}
infert.dt <- data.table(infert, stringsAsFactors = TRUE)
```

Fitting a linear regression model `M1` to predict secondary infertility using age and parity as predictors:
```{r}
M1 <- glm(case ~ age + parity, data = infert.dt, family="binomial")
```

Using the deviance to judge the goodness of fit of the model and report a p-value (3 significant digits):
```{1}
M1$deviance
```
The deviance of a model is defined as minus two times the log-likelihood of a model. Hence, the better the fit of a model, the lower the deviance.
However, the deviance alone doesn't tell us much about the goodness of fit of the model, we usually compare it to the deviance of another model. Since we don't have any model at this stage, I chose to compare it to the deviance of the null model. That is a model that bases its predictions only on the intercept term and hence its predicted probabilities are equal to the proportion of cases.
Since the difference between the deviances of two models is χ2 distributed (with d.f. equal to the difference in parameters) we can also report a p-value for the test:
```{r}
signif(pchisq(M1$null.deviance - M1$deviance, df=1, lower.tail=FALSE), 3)
```
Since the reported p-value 0.883 >> 0.05, we can't reject the hypothesis that the simpler model (null model) is better than `M1`.

#### Problem 4 (b)

Fitting a second model `M2` including the number of spontaneous abortions as a third predictors besides age and parity:
```{r}
M2 <- glm(case ~ age + parity + spontaneous, data = infert.dt, family="binomial")
```

Computing the odds ratio and 95% CI for the spontaneous abortions variable:
```{r}
M2.abortions <- exp(coef(M2)[4])
CI.M2.abortions <- exp(confint(M2)[4, ])
round(c(M2.abortions, CI.M2.abortions),2)
```

Reporting a p-value for the likelihood ratio test comparing model `M2` and `M1`
```{r}
pval <- pchisq(M1$deviance - M2$deviance, df=1, lower.tail=FALSE)
signif(pval, 2)
```
Since reported p-value 1.3e-09 << 0.05, we can conclude that the model `M2` that includes the number of spontaneous abortions is significantly better.

#### Problem (c)

Implementing a function that computes the binomial log-likelihood:
$$LogL(\beta) = \sum_{i \in case} \log p_{i} + \sum_{i \in ctrl} \log (1-p_{i}) $$
```{r}
loglik.binom <- function(y.obs, y.pred){
  
  # check input data
  stopifnot(length(y.obs) == length(y.pred))
  
  # use y.obs to split y.pred into cases and controls
  cases <- y.pred[which(y.obs == 1)]
  controls <- y.pred[which(y.obs == 0)]
  
  # compute log likelihood
  loglik <- sum(log(cases)) + sum(log(1-controls))
  
  # return log likelihood
  return(loglik)
}
```

Using function `loglik.binom()` to compute deviance for model `M2`:
```{r}
# extract observations from infert data set
obs <- infert.dt$case
# obtain predicted probabilities with predict() function
preds <- predict(M2, newdata = infert.dt, type = "response")

# deviance is minus two times the log likelihood of a model
M2.dev <- -2*loglik.binom(obs, preds) 
M2.dev

# check result
round(M2.dev, 2) == round(M2$deviance, 2)
```

Using function `loglik.binom()` to compute null deviance for model `M2`:
```{r}
# use the same observations as before 
null.obs <- obs
# use the same predicted probabilities - corresponding to the proportion of cases - for all observations
null.preds <- rep(sum( obs == 1)/length(obs), length((obs)))

# deviance is minus two times the log likelihood of a model
M2.null.dev <- -2*loglik.binom(null.obs, null.preds)
M2.null.dev

# check result
round(M2.null.dev, 2) == round(M2$null.deviance, 2)
```

#### Problem (d)

Using functions glm.cv() and predict.cv() to perform 10-folds cross-validation for model `M2`:
```{r}
# glm.cv function
glm.cv <- function(formula, data, folds) {
  regr.cv <- NULL
  for (fold in 1:length(folds)){
    regr.cv[[fold]] <- glm(formula, data=data[-folds[[fold]], ], family="binomial")
  }
  return(regr.cv)
}

# predict.cv function
predict.cv <- function(regr.cv, data, outcome, folds){
  pred.cv <- NULL
  for (fold in 1:length(folds)) {
    test.idx <- folds[[fold]]
    pred.cv[[fold]] <- data.frame(obs = outcome[test.idx], 
                                  pred = predict(regr.cv[[fold]],newdata=data,type="response")[test.idx])
  }
  return(pred.cv)
}

# load library
library(caret)

# set random seed to 1
set.seed(1)

# creating 10 folds
k <- 10
folds <- createFolds(infert.dt$case, k)

# fitting a logistic regression model in each of the folds
M2.cv <- glm.cv(case ~ age + parity + spontaneous, infert.dt, folds)

# validating M2 across the 10 folds
M2.cv.pred <- predict.cv(M2.cv, infert.dt, infert.dt$case, folds)
```

Evaluating the predictive performance of model M2:
Using function `loglik.binom()` to compute the log-likelihood of the predicted probabilities for each test fold:
```{r}
loglik <- numeric(k)

for (fold in 1:k){
  obs <- M2.cv.pred[[fold]]$obs
  pred <- M2.cv.pred[[fold]]$pred
  loglik[fold] <- round(loglik.binom(obs, pred),2)
  cat("Test fold ", fold, ": \t", loglik[fold],"\n")
}
```

Reporting the sum of the test log-likelihoods over all folds:
```{r}
sum(loglik)
```